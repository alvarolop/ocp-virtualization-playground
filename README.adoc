= OpenShift Virtualization Playground
Álvaro López Medina <alopezme@redhat.com>
v1.0, 2023-02
// Metadata
:description: This repository provides basic information about how to begin your journey with Openshift Virtualization on an OpenShift Container Platform installation.
:keywords: openshift, virtualization, red hat
// Create TOC wherever needed
:toc: macro
:sectanchors:
:sectnumlevels: 2
:sectnums: 
:source-highlighter: pygments
:imagesdir: docs/images
// Start: Enable admonition icons
ifdef::env-github[]
:tip-caption: :bulb:
:note-caption: :information_source:
:important-caption: :heavy_exclamation_mark:
:caution-caption: :fire:
:warning-caption: :warning:
// Icons for GitHub
:yes: :heavy_check_mark:
:no: :x:
endif::[]
ifndef::env-github[]
:icons: font
// Icons not for GitHub
:yes: icon:check[]
:no: icon:times[]
endif::[]

// Create the Table of contents here
toc::[]

== Introduction

This repository provides basic information about how to begin your journey with Openshift Virtualization on an OpenShift Container Platform installation. *All the examples and configuration are prepared for an OCP installation on AWS*. 

*OpenShift Virtualization* is an add-on to OpenShift Container Platform that allows you to *run and manage virtual machine workloads alongside container workloads*. More information https://docs.openshift.com/container-platform/4.12/virt/about-virt.html[here].

.OpenShift Kubernetes Engine
NOTE: https://docs.openshift.com/container-platform/4.12/welcome/oke_about.html[Red Hat OpenShift Kubernetes Engine] is a product offering from Red Hat that lets you use an enterprise-class Kubernetes platform as a production platform for launching containers. Openshift Virtualization is supported on OKE.


== TL;DR

This repository contains some basic examples of VM configuration for Openshift Virtualization. Maybe you have already configured it and just want to use the VMs in your deployments. If this is the case, please, take a look directly at the VM configurations:

[cols="5*",options="header",width=100%]
|===
|Template
|Networking
|Configuration
|Storage
|Location

| Fedora-01
a|{yes}Pod Network

{no}Multus
a| {no}Secrets

{no}ConfigMap
a| {no}DataVolume

{no}SharedFS
|link:virt-vms/01-vm-fedora.yaml[Click here]

| Fedora-02
a| {yes}Pod Network

{yes}Multus
a| {no}Secrets

{no}ConfigMap
a| {no}DataVolume

{no}SharedFS
|link:virt-vms/02-vm-fedora.yaml[Click here]

| Fedora-03
a| {yes}Pod Network

{yes}Multus
a| {yes}Secrets

{yes}ConfigMap
a| {no}DataVolume

{no}SharedFS
|link:virt-vms/03-vm-fedora.yaml[Click here]

| Fedora-04
a| {yes}Pod Network

{no}Multus
a| {no}Secrets

{no}ConfigMap
a| {yes}DataVolume

{no}SharedFS
|link:virt-vms/04-vm-fedora.yaml[Click here]
| Fedora-05
a| {yes}Pod Network

{no}Multus
a| {no}Secrets

{no}ConfigMap
a| {no}DataVolume

{yes}SharedFS
|link:virt-vms/05-vm-fedora.yaml[Click here]
|===


== Prerequisites for AWS

In order to be able to create VMs on OpenShift on AWS, you will need to add bare metal nodes to your OpenShift installation. The following steps simplify the process of creating a Bare Metal MachineSet:


1. Get the clusterID using the following command:
+
[source, bash]
----
oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster
----
+
2. Create the Machine Set:
+
[source, bash]
----
oc process -f ocp-installation/01-baremetal-machineset.yaml \
-p INFRASTRUCTURE_ID=$(oc get -o jsonpath='{.status.infrastructureName}{"\n"}' infrastructure cluster) \
| oc apply -n openshift-machine-api -f -
----

.*AWS Instances*
[TIP]
====
For other sizes of AWS nodes, check the https://aws.amazon.com/ec2/instance-types[instance types list].

Take into account that pricing for Bare Metal nodes is considerably higher than for normal instances. For example, `c5.metal` costs $4.8 per hour while `m5.xlarge` is just $0.222. https://aws.amazon.com/ec2/pricing/on-demand/[Here] you can find a full list of prices.
====

For more information, check the https://docs.openshift.com/container-platform/4.12/machine_management/creating_machinesets/creating-machineset-aws.html#machineset-yaml-aws_creating-machineset-aws[official documentation].



=== Adding a second interface to the node

Before creating the Network Configuration in your Bare Metal node, it needs to have an extra Network Interface to use for that purpose. The documentation of how to do it is in the https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html#working-with-enis[AWS documentation].

In general, you will have to access https://eu-west-3.console.aws.amazon.com/ec2/home?region=eu-west-3#CreateNetworkInterface[the Network Interfaces configuration] for your current region (In my case eu-west-3) and create a Network interface using the following configuration:

* Description: `Secondary network interface for bare metal node`.
* Subnet: Select the private network for the same region of the bare metal node `eu-west-3`.
* [Optional] Security Groups: You can add the `Security group for Kubernetes ELB` to your interface.

Click on `Create network interface` and access the https://eu-west-3.console.aws.amazon.com/ec2/home?region=eu-west-3#Instances:instanceState=running[nodes section], click on the metal node and then on the `Networking` tab. Attach the new interface to it.


This is automated with the following script:
[source, bash]
----
./ocp-tools/add-net-interface.sh ./aws-env-vars
----


Documentation:

* https://docs.aws.amazon.com/cli/latest/reference/ec2/create-network-interface.html[AWS docs for create-network-interface].
* https://docs.aws.amazon.com/cli/latest/reference/ec2/attach-network-interface.html[AWS docs for attach-network-interface].



== Installation

Red Hat OpenShift Virtualization is a Kubernetes operator on RHOCP, based on the Kubernetes virtualization add-on, KubeVirt, which allows the management of VM workloads alongside container workloads.

WARNING: The cluster must be installed on-premise, on bare metal with Red Hat Enterprise Linux CoreOS workers. AWS is not supported yet.

.*Disambiguation*
[TIP]
====
* *Virtualization* is to create virtualized resources from physical hardware, such as VMs.
* *IaaS* is a form of cloud computing that provides IT infrastructure.
* *Hypervisor* is virtualization software that helps you to create and manage VMs.
====

1. Create the `Project`, an `OperatorGroup` for the OLM, and a `Subscription` to install the operator:
+
[source, bash]
----
oc apply -f virt-installation/01-subscription.yaml
----
+
2. Create the `HyperConverged` object, which deploys and manages OpenShift Virtualization and its components:
+
[source, bash]
----
oc apply -f virt-installation/02-hyperconverged.yaml
----
+
3. Create the Project to deploy the VMs:
+
[source, bash]
----
oc process -f virt-installation/10-project.yaml | oc apply -f - 
----

If you install the operator using the web console, you will see the following messages during installation:

image::ocp-virt-installation.png[]

Click on `Create HyperConverged` button to create a default HyperConverged instance to be able to create Virtual Machines.




=== Virtualization CRDs

These are the CRDs that you can interact with in the `Installed Operators` section: 

* [HC] *OpenShift Virtualization Deployment* (HyperConverged) to deploy and manage OpenShift Virtualization and its components, such as the `virt-controller` cluster-level component and the `virt-handler` host-level Daemonset.
* [HPP] *HostPathProvisioner deployment* (HostPathProvisioners) to create virtual machines that use local node storage. (Not used in this repo).

As you can see, most of the CRDs are not here and you will find them in the new Dynamic Plugin navigation bar on the left of the Web Console. 




== Creating your first Virtual Machine

A *VM object* specifies a template to create a running instance of the VM inside your cluster. The running instance of a VM is a *virtual machine instance (VMI*), and it is executed and managed by a container located inside a pod. If a VMI is deleted, another instance is generated based on the VM object configuration.

The default templates are provided by Red Hat. These templates include settings to create generic systems with networking, users, and storage preconfigured. Create the Virtual Machine:

[source, bash]
----
oc process -f virt-vms/01-vm-fedora.yaml | oc apply -f -
----




=== Accessing the VM

The easiest way to SSH the VMs is using the *KubeVirt command line interface*. You can install it by downloading the binary from the OCP cluster or using the official the https://docs.openshift.com/container-platform/4.12/virt/virtual_machines/virt-accessing-vm-consoles.html[documentation].

Now, you can SSH the VM using the following command:

[source, bash]
----
virtctl -n vms-test ssh fedora@fedora-01
----

You can also access locally a service of the VM forwarding the port to your machine: 

[source, bash]
----
oc port-forward $VIRT_LAUNCHER_POD $REMOTE_PORT:$LOCAL_PORT -n $VM_PROJECT
----

Finally, you can perform extra configuration to automatically add your SSH Public Key to the VM on startup. Check the https://docs.openshift.com/container-platform/4.12/virt/virtual_machines/virt-accessing-vm-consoles.html#virt-accessing-vmi-ssh_virt-accessing-vm-consoles[documentation] for more information. Use the following command to set the `authorization-keys` on the server:

[source, console]
----
oc create secret generic user-pub-key --from-file=key1=$HOME/.ssh/id_rsa.pub -n vms-test
----





== Networking

You can connect a VM to three different types of networks:

* *Default pod network*: To use the default pod network, the network interface must use the Masquerade binding method. A masquerade binding uses NAT to allow other pods in the cluster to communicate with the VMI. 
* *Multus*: Connect a VM to multiple interfaces and external networks with the Container Networking Interface (CNI) plug-in, *Multus*. To connect to an external network, you must create a `linux-bridge` network attachment definition that exposes the layer-2 device to a specific namespace.
* *Single Root I/O Virtualization*: To connect to a virtual function network for high performance.

When the VMI is provisioned, the `virt-launcher` pod routes IPv4 traffic to the Dynamic Host Configuration Protocol (DHCP) address of the VMI. This routing makes it possible to also connect to a VMI with a port-forwarding connection.

Now, you have access to the pod network. Do you also want to add a second network to the VM? Great! You will have to use Multus, the NMstate operator and other great projects, so keep reading!




=== The NMstate operator

The Kubernetes NMState Operator provides a Kubernetes API for performing *state-driven network configuration* across the OpenShift Container Platform cluster's nodes with NMState. 

Red Hat OpenShift Virtualization uses the Kubernetes NMState Operator *to report on and configure node networking in a declarative way*. The Kubernetes NMstate Operator provides the components for declarative node networking in a Red Hat OpenShift cluster.

You can install it by applying the following file:

[source, bash]
----
# Install the operator 
oc apply -f ocp-installation/10-nmstate-installation.yaml
# Wait until the operator is installed
sleep 20
# Create the NMstate object
oc apply -f ocp-installation/11-nmstate-deployment.yaml
----

After that, it will be useful basically for three things:

1. Check the network configuration for each node using the *Node Network State (NNS)*:
+
[source, bash]
----
# Check all the network configurations:
oc get nns
# get the network configuration of an OCP node:
oc get nns $NODE_NAME -o yaml
----
+
2. Apply new configuration to nodes based on a selector using the *Node Network Configuration Policy (NNCP)*:
+
[source, bash]
----
oc apply -f virt-network/01-nncp-fedora.yaml
----
+
3. You can see the Configuration Policies with the following command:
+
[source, bash]
----
oc get nodenetworkconfigurationpolicy.nmstate.io
----
+
4. Finally, after completed successfully, you will see a report in a new object, the *Node Network Configuration Enactment (NNCE)*:
+
[source, bash]
----
oc get NodeNetworkConfigurationEnactment
----
+
5. If something is misconfigured, you can see the error message with the following command:
+
[source, bash]
----
oc get nnce $NODE_NAME -o jsonpath='{.status.conditions[?(@.type=="Failing")].message}'
----

NOTE: In order to apply this configuration only to Bare Metal nodes, we are labeling nodes with `usage: virtualization` in the MachineSet that we created in the first section. For more information, https://access.redhat.com/solutions/5802541[this KCS].

NOTE: If you need more information about this topic, you can check the https://docs.openshift.com/container-platform/4.12/networking/k8s_nmstate/k8s-nmstate-about-the-k8s-nmstate-operator.html[official documentation] for the NMstate Operator.

If you want to compare the configuration before and after setting the Node Network Configuration Policy, you can compare the files that contain the following outputs:

* `docs/examples/metal-node-nns-out-v01.yaml`: Before setting the configuration, there is no Bridge `br1`.
* `docs/examples/metal-node-nns-out-v02.yaml`: After setting the configuration, there is a Bridge named `br1`.






=== Multus 

The Multus CNI plug-in acts as a wrapper by calling other CNI plug-ins for advanced networking functionalities, such as *attaching multiple network interfaces* to pods in an OpenShift cluster.

How to configure it? Use the **Network Attachment Definition**, which is a namespaced object that exposes existing layer-2 network devices, such as bridges and switches, to VMs and pods.


[source, bash]
----
oc process -f virt-network/10-network-fedora-external.yaml | oc apply -f -
----


=== Creating a VM attached to 2 Networks

Create the Virtual Machine:

[source, bash]
----
oc process -f virt-vms/02-vm-fedora.yaml -p VM_NAME=fedora-02-a -p IP_ADDRESS="192.168.51.150/24" | oc apply -f -
oc process -f virt-vms/02-vm-fedora.yaml -p VM_NAME=fedora-02-b -p IP_ADDRESS="192.168.51.151/24" | oc apply -f -
----


== Application Configuration

Many applications require configuration using some combination of configuration files, command line arguments, and environment variables. Both `ConfigMaps` and `Secrets` are used to provide configuration settings and credentials to Pods.

The following template shows how to create a Secret and a ConfigMap and mount it as a file inside the VM:

[source, bash]
----
oc process -f virt-vms/03-vm-fedora.yaml -p VM_NAME=fedora-03 -p IP_ADDRESS="192.168.51.152/24" | oc apply -f -
----








== Storage

OCP-VIRT provides several mechanisms to manage the VM disks. It introduces new resource types to facilitate the process of creating the PVC with optimal parameters for VM disks and copying the disk image into the resulting PV:

* *StorageProfile*: For each storage class, a StorageProfile resource gives default values optimized for VM disks. As a developer, when you use a storage profile to prepare a VM disk, the only parameter that you must provide is the disk size.

* *DataVolume*: A DataVolume resource describes a VM disk. It groups the PVC definition and the details of the disk image to inject into the PV.


=== Extra disks using Data Volumes

DataVolume resources have two parts:

* The *storage profile* specification, which provides the details of the PVC to create. You only need to specify the disk size.
* The *source image* details, which provides the disk image to inject into the PV.

==== Disk type

The disk type inside the VM depends on the interface that you select when you attach the data volume:

* `scsi` interface: Standard SCSI device. Linux systems name it with the `/dev/sdX` format.
* `virtio` interface: [Optimal performance] Linux systems name it with the `/dev/vdX` format. Some operating systems do not provide that driver by default.

NOTE: When you hot plug a disk to a running VM, `scsi` is the only available interface.

==== Data Volume Source

The source section of a DataVolume resource provides the details of the disk image to inject into the persistent volume (PV).

* Blank (creates PVC).
* Import via URL (creates PVC).
* Use an existing PVC.
* Clone existing PVC (creates PVC).
* Import via Registry (creates PVC).
* Container (ephemeral).



=== Creating a VM with an extra block disk

Adding an extra block disk is as simple as creating a `DataVolume` with `.spec.source.blank: {}` and attach it to the VM. In the Template I also add the commands to generate the filesystem in the cloud-init for the sake of simplicity:

[source, bash]
----
# Create a VM and its blank disk at the same time
oc process -f virt-vms/04-vm-fedora.yaml -p VM_NAME=fedora-04 | oc apply -f -
----



=== Creating two VMs with a shared filesystem

WARNING: Currently, this chapter is not working as expected as `virtiofs` is an Experimental feature for Kubevirt and, without it, you cannot mount File Systems.


==== Adding a filesystem Storage Class

The only StorageClass available by default on OCP on AWS is GP-2 and GP-3 which are AWS Elastic Block Store. This does not allow us to create RWX File Systems. Therefore, we have to add the https://docs.openshift.com/container-platform/4.12/storage/container_storage_interface/persistent-storage-csi-aws-efs.html[AWS Elastic File Service CSI Driver Operator] to access EFS or https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.12[ODF (Openshift Data Foundation)].

Access this documentation to know an automated process to configure AWS EFS in an OCP cluster deployed on AWS:

>> link:storage-csi-aws-efs/README.adoc[Click Here] <<

Access this documentation to know an automated process to configure Openshift Data Foundation on OCP on AWS:

>> link:storage-odf-aws/README.adoc[Click Here] <<

==== Creating the OCP resources

The following commands allow you to create two VMs using the new Storage Class:

.Create Data Volume using AWS EFS
[source, bash]
----
oc process -f virt-vms/05-vm-shared-disk.yaml | oc apply -f -
----

.Create Data Volume using ODF CephFS
[source, bash]
----
oc process -f virt-vms/05-vm-shared-disk.yaml -p STORAGE_CLASS_NAME=ocs-storagecluster-cephfs | oc apply -f -
----

.Create both VMs
[source, bash]
----
oc process -f virt-vms/05-vm-fedora.yaml -p VM_NAME=fedora-05-a | oc apply -f -
oc process -f virt-vms/05-vm-fedora.yaml -p VM_NAME=fedora-05-b | oc apply -f -
----



== Observability

=== Logging

=== Metrics

image::ocp-virt-vm-dashboard.png[]


image::ocp-virt-vm-metrics.png[]








:sectnums!:

== Annex: Testing container image

To quickly deploy a container with tools to check connectivity, I normally use the UBI version of the *Red Hat Enterprise Linux Support Tools* which can be found in the https://catalog.redhat.com/software/containers/rhel8/support-tools/5ba3eaf9bed8bd6ee819b78b?container-tabs=overview[RH Container Catalog]. 

You can deploy this container using the following script:

[source, bash]
----
oc process -f ocp-tools/01-toolbox.yaml -p POD_PROJECT=vms-test | oc apply -f -
----


== Analysis: Networking configuration

In some cases, networking configuration could be tricky. That's why in this document I compare several VM configuration combinations and their real configuration in the machine.

>> link:docs/analysis-network-config/RESULTS.adoc[Click Here] <<



== Additional documentation

* KCS: https://access.redhat.com/articles/6409731[Deploy OpenShift Virtualization on AWS metal instance types].
* KCS: https://access.redhat.com/articles/6738351[Deploy OpenShift sandboxed containers on AWS Bare Metal nodes (Tech Preview)].
* KCS: https://access.redhat.com/articles/6994974[OpenShift Virtualization - Tuning & Scaling Guide].
* RH Blog: https://cloud.redhat.com/blog/openshift-virtualization-on-amazon-web-services[OpenShift Virtualization on Amazon Web Services].